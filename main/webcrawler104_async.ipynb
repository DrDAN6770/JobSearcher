{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Moduel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re, time, requests\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# import grequests # 看起要在.py才能用\n",
    "from aiohttp import ClientSession\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 非同步版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class eJob_search104():\n",
    "    current_date = datetime.now().date()\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    search_url = 'https://www.104.com.tw/jobs/search/?'\n",
    "\n",
    "    def __init__(self, filter_params, key_word, page = 15):\n",
    "        self.filter_params = filter_params\n",
    "        self.key_word = key_word\n",
    "        self.page = page\n",
    "           \n",
    "    def search_total_jobs_link(self) -> list[str]:\n",
    "        url = requests.get(self.search_url, self.filter_params, headers=self.headers).url\n",
    "        print(url)\n",
    "        option = Options()\n",
    "        option.add_argument(f\"user-agent={self.headers['User-Agent']}\")\n",
    "        option.add_experimental_option('excludeSwitches', ['enable-automation']) # 開發者模式。可以避開某些防爬機制，有開有保佑\n",
    "        # option.add_argument('--headless') # 無頭模式，開發完成之後再使用，可以完全背景執行，有機會變快\n",
    "        # option.add_argument(\"--disable-gpu\") # 禁用GPU加速，有些情況下需要設置這個參數\n",
    "        driver = webdriver.Chrome(options=option)\n",
    "        driver.get(url)\n",
    "\n",
    "        element = driver.find_element(By.XPATH,'//*[@id=\"js-job-header\"]/div[1]/label[1]/select/option[1]')\n",
    "        total_page = int(re.sub(r'\\D', '', element.text.split('/')[-1]))\n",
    "        print(f'Total_page = {total_page}')\n",
    "        # 滾頁面\n",
    "        scroll_times = self.page\n",
    "        for _ in range(scroll_times):\n",
    "            driver.execute_script('window.scrollTo(0,document.body.scrollHeight);')\n",
    "            time.sleep(2)\n",
    "\n",
    "        # 自動加載結束後要自行點選載入(15以後)\n",
    "        # 使用CSS選擇器定位最後一個按鈕並點擊\n",
    "        if total_page >= 15:\n",
    "            k = 1\n",
    "            while True:\n",
    "                try:\n",
    "                    button_element = WebDriverWait(driver, 4).until(\n",
    "                        EC.element_to_be_clickable((By.CSS_SELECTOR, '#js-job-content > div:last-child > button'))\n",
    "                    )\n",
    "                    print(f'手動載入第{15 + k}頁')\n",
    "                    button_element.click()\n",
    "                    k += 1\n",
    "                    if k == 86 or k == total_page - 14 :\n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    print(\"發生未知錯誤：\", e)\n",
    "                    break\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        raw_Job_list = soup.find_all(\"a\",class_=\"js-job-link\")\n",
    "        print(f'共{len(raw_Job_list)}筆資料')\n",
    "        driver.quit()\n",
    "        return raw_Job_list\n",
    "    \n",
    "    def filter_jobs_link(self, raw_Job_list : list) -> list[str]:\n",
    "        filter_job_list = [i for i in raw_Job_list if re.search(self.key_word, i['title'].lower())]\n",
    "        print(f'過濾完有{len(filter_job_list)}筆')\n",
    "        return filter_job_list\n",
    "\n",
    "    async def getAPI_respone(self, url : str) -> dict:\n",
    "        pattern = r'job/(.*?)\\?jobsource'\n",
    "        match = re.search(pattern, url)\n",
    "        if match:\n",
    "            apiurl = 'https://www.104.com.tw/job/ajax/content/' + match.group(1)\n",
    "            header = {\n",
    "                'Accept':'application/json, text/plain, */*',\n",
    "                'Accept-Encoding':'gzip, deflate, br',\n",
    "                'Accept-Language':'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7',\n",
    "                'Connection':'keep-alive',\n",
    "                'Host':'www.104.com.tw',\n",
    "                'Referer':url,\n",
    "                'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "            }\n",
    "            async with ClientSession(headers=header) as session:\n",
    "                async with session.get(apiurl) as response:\n",
    "                    # response.raise_for_status()\n",
    "                    return await response.json()\n",
    "        return {}\n",
    "\n",
    "    async def get_Job_info_fromAPI(self, item : str) -> pd.DataFrame:\n",
    "        url =  f\"https:{item['href']}\"\n",
    "        alldata = await self.getAPI_respone(url)\n",
    "\n",
    "        if not alldata:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            res = {}\n",
    "            res['更新日期'] = [alldata['data']['header']['appearDate']]\n",
    "            res['職缺名稱'] = [alldata['data']['header']['jobName']]\n",
    "            res['公司名稱'] = [alldata['data']['header']['custName']]\n",
    "            res['工作內容'] = [alldata['data']['jobDetail']['jobDescription']]\n",
    "            res['職務類別'] = ['、'.join(job_category['description'] for job_category in alldata['data']['jobDetail']['jobCategory'])]\n",
    "            res['工作待遇'] = [alldata['data']['jobDetail']['salary']]\n",
    "            res['工作性質'] = [alldata['data']['jobDetail']['jobType']]\n",
    "            res['上班地點'] = [alldata['data']['jobDetail']['addressRegion'] + alldata['data']['jobDetail']['addressDetail']]\n",
    "            res['管理責任'] = [alldata['data']['jobDetail']['manageResp']]\n",
    "            res['出差外派'] = [alldata['data']['jobDetail']['businessTrip']]\n",
    "            res['上班時段'] = [alldata['data']['jobDetail']['workPeriod']]\n",
    "            res['休假制度'] = [alldata['data']['jobDetail']['vacationPolicy']]\n",
    "            res['可上班日'] = [alldata['data']['jobDetail']['startWorkingDay']]\n",
    "            res['需求人數'] = [alldata['data']['jobDetail']['needEmp']]\n",
    "\n",
    "            res['工作經歷'] = [alldata['data']['condition']['workExp']]\n",
    "            res['學歷要求'] = [alldata['data']['condition']['edu']]\n",
    "\n",
    "            major = '、'.join(major for major in alldata['data']['condition']['major'])\n",
    "            res['科系要求'] = [major if major else '不拘']\n",
    "\n",
    "            language = '、'.join(lang['language'] for lang in alldata['data']['condition']['language'])\n",
    "            res['語文條件'] = [language if language else '不拘']\n",
    "\n",
    "            specialty = '、'.join(specialty['description'] for specialty in alldata['data']['condition']['specialty'])\n",
    "            res['擅長工具'] = [specialty if specialty else '不拘']\n",
    "\n",
    "            skill = '、'.join(skill['description'] for skill in alldata['data']['condition']['skill'])\n",
    "            res['工作技能'] = [skill if skill else '不拘']\n",
    "\n",
    "            other = alldata['data']['condition']['other']\n",
    "            res['其他要求'] = [other if other else '不拘']\n",
    "            res['連結'] = [url]\n",
    "            \n",
    "            return pd.DataFrame(res)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(\"發生錯誤\", e)\n",
    "            return None\n",
    "\n",
    "    async def scrape(self, Job_list : list):\n",
    "        tasks = []\n",
    "        semaphore = asyncio.Semaphore(10)  # Limit concurrent requests to 10\n",
    "\n",
    "        for item in Job_list:\n",
    "            async with semaphore:\n",
    "                task = asyncio.ensure_future(self.get_Job_info_fromAPI(item))\n",
    "                tasks.append(task)\n",
    "\n",
    "        return await asyncio.gather(*tasks)\n",
    "    \n",
    "    def main(self, Job_list: list, DF):\n",
    "        batch_size = 30\n",
    "        num_batches = (len(Job_list) + batch_size - 1) // batch_size\n",
    "        for batch_idx in range(num_batches):\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = min((batch_idx + 1) * batch_size, len(Job_list))\n",
    "            Job_list_batch = Job_list[start_idx:end_idx]\n",
    "            loop = asyncio.get_event_loop()\n",
    "            results = loop.run_until_complete(self.scrape(Job_list_batch))\n",
    "            # loop.close()\n",
    "\n",
    "            for df in results:\n",
    "                if df is not None:\n",
    "                    DF = pd.concat([DF, df], ignore_index=True)\n",
    "\n",
    "        DF.to_csv(f\"../output/JBLIST_{self.current_date}.csv\", sep=',', index=False)\n",
    "        return DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 執行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.104.com.tw/jobs/search/?ro=1&keyword=%E8%B3%87%E6%96%99%E5%B7%A5%E7%A8%8B&area=6001002000%2C6001001000%2C6001006000%2CC6001008000&isnew=7&jobexp=1%2C3&mode=l&order=16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The chromedriver version (119.0.6045.105) detected in PATH at c:\\Users\\DAN\\source\\VScode\\Python\\Web_Crawler\\Scripts\\chromedriver.exe might not be compatible with the detected chrome version (120.0.6099.217); currently, chromedriver 120.0.6099.109 is recommended for chrome 120.*, so it is advised to delete the driver in PATH and retry\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total_page = 97\n",
      "手動載入第16頁\n",
      "手動載入第17頁\n",
      "手動載入第18頁\n",
      "手動載入第19頁\n",
      "手動載入第20頁\n",
      "手動載入第21頁\n",
      "手動載入第22頁\n",
      "手動載入第23頁\n",
      "手動載入第24頁\n",
      "手動載入第25頁\n",
      "手動載入第26頁\n",
      "手動載入第27頁\n",
      "手動載入第28頁\n",
      "手動載入第29頁\n",
      "手動載入第30頁\n",
      "手動載入第31頁\n",
      "手動載入第32頁\n",
      "手動載入第33頁\n",
      "手動載入第34頁\n",
      "手動載入第35頁\n",
      "手動載入第36頁\n",
      "手動載入第37頁\n",
      "手動載入第38頁\n",
      "手動載入第39頁\n",
      "手動載入第40頁\n",
      "手動載入第41頁\n",
      "手動載入第42頁\n",
      "手動載入第43頁\n",
      "手動載入第44頁\n",
      "手動載入第45頁\n",
      "手動載入第46頁\n",
      "手動載入第47頁\n",
      "手動載入第48頁\n",
      "手動載入第49頁\n",
      "手動載入第50頁\n",
      "手動載入第51頁\n",
      "手動載入第52頁\n",
      "手動載入第53頁\n",
      "手動載入第54頁\n",
      "手動載入第55頁\n",
      "手動載入第56頁\n",
      "手動載入第57頁\n",
      "手動載入第58頁\n",
      "手動載入第59頁\n",
      "手動載入第60頁\n",
      "手動載入第61頁\n",
      "手動載入第62頁\n",
      "手動載入第63頁\n",
      "手動載入第64頁\n",
      "手動載入第65頁\n",
      "手動載入第66頁\n",
      "手動載入第67頁\n",
      "手動載入第68頁\n",
      "手動載入第69頁\n",
      "手動載入第70頁\n",
      "手動載入第71頁\n",
      "手動載入第72頁\n",
      "手動載入第73頁\n",
      "手動載入第74頁\n",
      "手動載入第75頁\n",
      "手動載入第76頁\n",
      "手動載入第77頁\n",
      "手動載入第78頁\n",
      "手動載入第79頁\n",
      "手動載入第80頁\n",
      "手動載入第81頁\n",
      "手動載入第82頁\n",
      "手動載入第83頁\n",
      "手動載入第84頁\n",
      "手動載入第85頁\n",
      "手動載入第86頁\n",
      "手動載入第87頁\n",
      "手動載入第88頁\n",
      "手動載入第89頁\n",
      "手動載入第90頁\n",
      "手動載入第91頁\n",
      "手動載入第92頁\n",
      "手動載入第93頁\n",
      "手動載入第94頁\n",
      "手動載入第95頁\n",
      "手動載入第96頁\n",
      "手動載入第97頁\n",
      "共2890筆資料\n",
      "過濾完有1927筆\n",
      "花費 405.802006483078 秒\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# 過濾關鍵字以外的職缺\n",
    "keywords_pattern = r'工程|資料|python|data|數據'\n",
    "\n",
    "# 搜尋關鍵字\n",
    "filter_params = {\n",
    "    'ro' : 1, # 1 全職\n",
    "    'keyword' : '資料工程',\n",
    "    'area' : '6001002000,6001001000,6001006000,C6001008000',  # 6001001000 台北市 6001002000 新北 6001006000 新竹縣市 6001008000 台中市\n",
    "    'isnew' : 7, # 0:本日 3:3天內 7:1週內 14 30\n",
    "    'jobexp' : '1,3', # 工作經驗1年以下 + 1-3年\n",
    "    'mode' : 'l', # 列表模式(比較多筆資料)\n",
    "    'order' : 16 # 照日期排序\n",
    "}\n",
    "\n",
    "# 建立物件\n",
    "DF = pd.DataFrame()\n",
    "EJS = eJob_search104(filter_params, keywords_pattern)\n",
    "retry_count = 0\n",
    "while True:\n",
    "    try:\n",
    "        raw_Job_list = EJS.search_total_jobs_link()\n",
    "        break\n",
    "    except:\n",
    "        retry_count += 1\n",
    "        if retry_count == 3:\n",
    "            break\n",
    "        print(f'執行錯誤, retry {retry_count}')\n",
    "Job_list = EJS.filter_jobs_link(raw_Job_list)\n",
    "result_df = EJS.main(Job_list, DF)\n",
    "print(f\"花費 {time.time() - start_time} 秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing data\n",
    "result_df[result_df.isnull().any(axis=1)]\n",
    "clean_df = result_df.dropna()\n",
    "clean_df = clean_df.drop_duplicates()\n",
    "\n",
    "# city\n",
    "clean_df['縣市'] = clean_df['上班地點'].apply(lambda x:x[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 儲存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "current_date = datetime.now().date()\n",
    "if clean_df.isnull().sum().sum() == 0:\n",
    "    clean_df.to_csv(f\"../output/JBLIST_{current_date}.csv\", sep=',', index=False)\n",
    "    print(\"Done!\")\n",
    "else:\n",
    "    print(\"NaN exist\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Web_Crawler",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
